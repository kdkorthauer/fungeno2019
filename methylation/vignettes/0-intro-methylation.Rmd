---
title: Intro to DNA methylation sequencing analysis in R
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
---   

```{r, echo=FALSE, message=FALSE, results="hide", cache=FALSE}
library(BiocStyle)
knitr::opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE)
set.seed(651)
```


# Logistic (Binomial) regression 

Let's start with a very simple example, where we have two groups (goverened by $x$), each with a different probability of success. If $x=0$, then $p=0.2$, and if $x=1$ then $p=0.4$. 

```{r, fig.width=5, fig.height = 3.5}
library(ggplot2)
library(dplyr)

set.seed(1)
n <- 50
cov <- 10
x <- c(rep(0,n/2), rep(1, n/2))
p <- 0.4 + 0.2*x
y <- rbinom(n, cov, p)

ggplot(data.frame(x=factor(x),y=as.numeric(y)), 
            aes(x=x, y=y/cov)) +
  geom_point(position=position_jitter(height=0.02, width=0.07)) +
  xlab("x") + 
  ylab("y/cov") 
```

Now we fit a logistic regression model with $x$ as a covariate.

```{r}
# Fit a logistic regression model
model0 <- glm(cbind(y, cov-y) ~ x, family="binomial")
summary(model0)
```

We see that $x$ is very predictive of $y$, as we expect. 

***
## Exercise

Recall that the estimated probability of success for the logistic regression model is $$ e^{\beta_0 + x\beta_1}$$. For `model0`, find the estimated probability of success when $x=0$ and when $x=1$. 

```{r}
# your code here
```

***

# Logistic regression with overdispersion

Now, let's add in some beta-distributed noise. Here, we let the probabilities used by the binomial sampling be equal to the probabilities $(1-x)p_0 + xp_1$, where $p_i \sim Beta(\alpha_i,\beta_i)$. We let $\alpha_0=4$ and $\beta_0=6$ (when $x=0$). Likewise we set $\alpha_1=6$ and $\beta_1=4$ (when $x=1$). Since the mean of the beta distribution $Beta(\alpha, \beta)$ is $$\frac{\alpha}{\alpha+\beta},$$ the average probability of success for the first group ($x=0$) is 0.4, and the average probability of success for the second group ($x=1$) is 0.6. This is because $$E[p] = (1-x)E[p_0] + xE[p_1].$$

Then, we plot the outcomes $y$ against the known value $x$.

```{r, fig.width=5, fig.height = 3.5}
set.seed(1)
n <- 50
cov <- 10
x <- c(rep(0,n/2), rep(1, n/2))
p <- pmin((1-x)*rbeta(n,4,6) + x*rbeta(n,6,4), 1)
y <- rbinom(n, cov, p)
 
ggplot(data.frame(x=factor(x),y=as.numeric(y), p), 
            aes(x=x, y=y/cov)) +
  geom_point(position=position_jitter(height=0.02, width=0.07)) +
  xlab("x") + 
  ylab("y") 
```

Now we fit a logistic regression model with $x$ as a covariate. Note that the additional beta noise is not modeled.

```{r}
# Fit a logistic regression model
model1 <- glm(cbind(y, cov-y) ~ x, family="binomial")
summary(model1)
```

Now we see that $x$ is still predictive of $y$, however the coefficient has a slightly larger _p_-value, likely influenced by the extra beta noise in the binomial probabilities.


***
## Exercise

For `model1`, find the estimated probability of success when $x=0$ and when $x=1$. 

```{r}
# your code here
```

***

## Bootstrap comparison 

This result is just based on one random sample - to convince ourselves that this is a consistent effect, we could repeat these two model fits on many random samples (e.g. using the [dplyr](https://cran.r-project.org/web/packages/dplyr/index.html), [broom](https://cran.r-project.org/web/packages/broom/index.html), and
[purrr](https://cran.r-project.org/web/packages/purrr/index.html)
packages. Here we show results for 10,000 replicates.

```{r, fig.width = 4, fig.height = 3.5}
library(dplyr)
library(broom)
library(purrr)

set.seed(10)

n <- 50
cov <- 10
B <- 1000
x <- c(rep(0,n/2), rep(1, n/2))

# functions to run one replicate for each model
m0_rep <- function(){
  data.frame(x = c(rep(0,n/2), rep(1, n/2)),
             p = 0.4 + 0.2*x,
             cov = cov,
             y=rbinom(n, cov, p)) %>%  
    do(tidy(glm(cbind(y, cov-y) ~ x, family="binomial", data=.), 
            conf.int = TRUE)) 
}

m1_rep <- function(){
  data.frame(x = c(rep(0,n/2), rep(1, n/2)),
             p <- pmin((1-x)*rbeta(n,4,6) + x*rbeta(n,6,4), 1),
             cov = cov,
             y=rbinom(n, cov, p)) %>%  
    do(tidy(glm(cbind(y, cov-y) ~ x, family="binomial", data=.), 
            conf.int = TRUE))
}

# replicate B times
m0_all <- replicate(B, m0_rep(), simplify=FALSE) %>% 
  do.call("rbind", .) %>%
  mutate(model="m0") %>%
  mutate(n = sort(rep(1:B, 2)))
m1_all <- replicate(B, m1_rep(), simplify=FALSE) %>% 
  do.call("rbind", .) %>%
  mutate(model="m1") %>%
  mutate(n = sort(rep(1:B, 2)))

# combine and pull out relevant info
all <- rbind(m0_all, m1_all)
x <- filter(all, term == "x")
prob <- all %>% group_by(model,n) %>%
  summarize(p = sum(estimate))

prob %>% ggplot(aes(x = model, y = p)) +
  geom_boxplot() +
  ylab("p estimate for x=1") + 
  abline(h=0.4, linetype="dashed", width=0.8, color = "blue")
  
all %>% ggplot(aes(x = model, y = -log10(p.value))) +
  geom_boxplot() +
  ylab("p-value estimate for x")
  
```



# Beta-binomial regression 

In order to account for the overdispersion in the binomial probabilities, let's try fitting a beta-binomial regression model to the data instead.

```{r}
library(aod)
dat <- data.frame(s = y, f = cov-y, x = factor(x))
model2 <- betabin(cbind(s,f) ~ x, ~ x, data=dat)
summary(model2)
```

Notice that the estimated coefficients are similar to `model1`. Also notice that the standard errors are larger, and therefore the _p_-value for the $x$ covariate is larger. We can also see that new overdispersion parameters ($\phi_{x=1}, \phi_{x=1}$) are estimated. Recall that in the beta-binomial regression model, $$ \phi = \frac{1}{\alpha+\beta+1} $$

***
## Exercise

For `model2`, find the estimated probability of success when $x=0$ and when $x=1$. 

```{r}
# your code here
```

What are the true values of the overdispersion parameters in this model? How close are the estimated overdispersion coefficients in `model2`?

```{r}
# your code here
```

***
